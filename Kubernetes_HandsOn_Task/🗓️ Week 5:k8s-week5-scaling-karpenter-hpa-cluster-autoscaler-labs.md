## ğŸ§­ Kubernetes Week 5: Scaling & Autoscaling with Karpenter, HPA, and Cluster Autoscaler â€“ 10 Hands-On Labs

### ğŸ“  Subtitle :  
> Dive into real-world Kubernetes autoscaling strategies using Karpenter, Horizontal Pod Autoscaler (HPA), and Cluster Autoscaler. Includes fullstack, backend, and frontend scenarios â€“ from basic CPU scaling to dynamic EC2 node provisioning, load simulation, and event-driven scaling with KEDA.

---

### âœ… Prompt 1: **HPA on NGINX Deployment Using CPU**  
**ğŸ§  Level:** Beginner  
```
ğŸ”§ Iâ€™m a DevOps beginner exploring autoscaling.  
ğŸ¯ I want to apply Horizontal Pod Autoscaler (HPA) to an NGINX deployment that scales based on CPU.  
ğŸ’» Stack: Kubernetes, HPA, NGINX  
ğŸ“ Structure:
hpa-nginx/
â”œâ”€â”€ deployment.yaml  
â”œâ”€â”€ hpa.yaml  

ğŸ”£ Input: `cpu: 50m`, min=1, max=3  
ğŸ“¤ Output: Pods auto-scale with CPU load  

ğŸŒ Environment: Minikube or AWS EKS  
ğŸ“œ Constraints: Must use Metrics Server  
ğŸ§¾ Output: HPA status via `kubectl describe hpa`  
ğŸ”§ Experience Level: Beginner  
ğŸ¢ Use Case: Learn Kubernetes HPA basics  
ğŸ’¡ I want YAML files + simulation instructions  
ğŸš€ Deploy: kubectl  
ğŸ” Best Practices: Always set `resources.requests`  
ğŸ“¥ Logs: `kubectl top pods`, `kubectl get hpa`  
ğŸŒ Language: English
```

---

### âœ… Prompt 2: **Autoscale Express Backend with Load Simulation**  
**ğŸ§  Level:** Intermediate  
```
ğŸ”§ Iâ€™m a backend engineer.  
ğŸ¯ I want to deploy a Node.js Express API and scale it using HPA with load generated via `hey`.  
ğŸ’» Stack: Node.js, Express, Kubernetes, HPA  
ğŸ“ Structure:
hpa-express/
â”œâ”€â”€ Dockerfile  
â”œâ”€â”€ app.js  
â”œâ”€â”€ deployment.yaml  
â”œâ”€â”€ hpa.yaml  

ğŸ”£ Input: `/api/ping` endpoint  
ğŸ“¤ Output: Pods scale up when simulated traffic spikes  

ğŸŒ Environment: AWS EKS or Minikube  
ğŸ“œ Constraints: CPU-based scaling only  
ğŸ§¾ Output: HPA reacts to load within 1-2 minutes  
ğŸ”§ Experience Level: Intermediate  
ğŸ¢ Use Case: Backend autoscaling under stress  
ğŸ’¡ I want working app + load simulation  
ğŸš€ Deploy: kubectl  
ğŸ” Best Practices: Use readiness probe for better scaling  
ğŸ“¥ Logs: `kubectl logs` + `top pods`  
ğŸŒ Language: English
```

---

### âœ… Prompt 3: **Deploy Karpenter on EKS Using Helm**  
**ğŸ§  Level:** Intermediate  
```
ğŸ”§ Iâ€™m setting up autoscaling infrastructure.  
ğŸ¯ I want to install Karpenter on EKS using Helm and configure a basic Provisioner.  
ğŸ’» Stack: AWS EKS, Helm, Karpenter  
ğŸ“ Structure:
karpenter-install/
â”œâ”€â”€ provisioner.yaml  
â”œâ”€â”€ karpenter-values.yaml  

ğŸ”£ Input: Spot + OnDemand instance types  
ğŸ“¤ Output: Karpenter scales EC2 nodes based on unscheduled pods  

ğŸŒ Environment: AWS EKS  
ğŸ“œ Constraints: Karpenter IAM role and controller must be in place  
ğŸ§¾ Output: EC2 nodes created by Karpenter  
ğŸ”§ Experience Level: Intermediate  
ğŸ¢ Use Case: Dynamic infra scaling  
ğŸ’¡ I want full Helm setup with YAML files  
ğŸš€ Deploy: Helm  
ğŸ” Best Practices: Use instance weighting and limits  
ğŸ“¥ Logs: `karpenter` controller logs  
ğŸŒ Language: English
```

---

### âœ… Prompt 4: **Autoscale React Frontend with CPU and Custom Metrics**  
**ğŸ§  Level:** Intermediate  
```
ğŸ”§ Iâ€™m a frontend engineer deploying on Kubernetes.  
ğŸ¯ I want to autoscale a React app based on CPU and page hit metrics (via Prometheus Adapter).  
ğŸ’» Stack: React, Prometheus, HPA, Custom Metrics  
ğŸ“ Structure:
hpa-react-metrics/
â”œâ”€â”€ react-app/  
â”œâ”€â”€ deployment.yaml  
â”œâ”€â”€ service.yaml  
â”œâ”€â”€ hpa.yaml  

ğŸ”£ Input: Hit count metric, CPU > 50%  
ğŸ“¤ Output: Replicas scale based on usage  

ğŸŒ Environment: EKS with Prometheus Adapter  
ğŸ“œ Constraints: Adapter must expose custom metric  
ğŸ§¾ Output: HPA status + pods increased  
ğŸ”§ Experience Level: Intermediate  
ğŸ¢ Use Case: UI with variable traffic  
ğŸ’¡ I want React app + custom metric config  
ğŸš€ Deploy: Helm + kubectl  
ğŸ” Best Practices: Use proper labels and selectors  
ğŸ“¥ Logs: Prometheus adapter log + HPA status  
ğŸŒ Language: English
```

---

### âœ… Prompt 5: **HPA with Memory-Based Scaling**  
**ğŸ§  Level:** Intermediate  
```
ğŸ”§ I want to explore memory-based autoscaling.  
ğŸ¯ I want to use HPA that scales pods based on memory instead of CPU.  
ğŸ’» Stack: Kubernetes, Metrics Server  
ğŸ“ Structure:
hpa-memory/
â”œâ”€â”€ deployment.yaml  
â”œâ”€â”€ hpa-memory.yaml  

ğŸ”£ Input: Memory threshold 200Mi  
ğŸ“¤ Output: App scales up with increased memory use  

ğŸŒ Environment: Minikube or EKS  
ğŸ“œ Constraints: Metrics Server must support memory  
ğŸ§¾ Output: `kubectl describe hpa` shows memory stats  
ğŸ”§ Experience Level: Intermediate  
ğŸ¢ Use Case: Apps with high memory but low CPU  
ğŸ’¡ I want clear memory-based HPA example  
ğŸš€ Deploy: kubectl  
ğŸ” Best Practices: Use limits to enforce boundaries  
ğŸ“¥ Logs: pod metrics + HPA events  
ğŸŒ Language: English
```

---

### âœ… Prompt 6: **Karpenter with Consolidation and Spot Preferences**  
**ğŸ§  Level:** Hard  
```
ğŸ”§ I want to optimize node usage dynamically.  
ğŸ¯ I want to configure Karpenter with consolidation and weighted instance types (favoring Spot).  
ğŸ’» Stack: Karpenter, AWS EKS  
ğŸ“ Structure:
karpenter-consolidate/
â”œâ”€â”€ provisioner.yaml  

ğŸ”£ Input: Consolidation and capacity type: `spot`  
ğŸ“¤ Output: Spot node scales in/out based on pod spec  

ğŸŒ Environment: EKS  
ğŸ“œ Constraints: Spot instance IAM policies  
ğŸ§¾ Output: Consolidation logs and scaling chart  
ğŸ”§ Experience Level: Hard  
ğŸ¢ Use Case: Cost-efficient auto-provisioning  
ğŸ’¡ I want a provisioner config with priorities  
ğŸš€ Deploy: Helm  
ğŸ” Best Practices: Set disruption budgets  
ğŸ“¥ Logs: `karpenter` logs and EC2 events  
ğŸŒ Language: English
```

---

### âœ… Prompt 7: **Use Cluster Autoscaler with Multiple Node Groups**  
**ğŸ§  Level:** Hard  
```
ğŸ”§ I want to scale nodes with Cluster Autoscaler.  
ğŸ¯ I want to configure CA to scale multiple node groups (spot + on-demand) in EKS.  
ğŸ’» Stack: AWS EKS, Cluster Autoscaler  
ğŸ“ Structure:
cluster-autoscaler/
â”œâ”€â”€ cluster-autoscaler.yaml  
â”œâ”€â”€ nodegroups.tf  

ğŸ”£ Input: NodeAffinity for pods  
ğŸ“¤ Output: CA adds/removes nodes per pod affinity  

ğŸŒ Environment: AWS EKS  
ğŸ“œ Constraints: Multiple ASGs, autoscaler deployment  
ğŸ§¾ Output: Node scale events  
ğŸ”§ Experience Level: Hard  
ğŸ¢ Use Case: Flexible node scaling  
ğŸ’¡ I want autoscaler config + IAM policy  
ğŸš€ Deploy: Terraform + kubectl  
ğŸ” Best Practices: Taint spot nodes for fallback  
ğŸ“¥ Logs: Cluster Autoscaler logs  
ğŸŒ Language: English
```

---

### âœ… Prompt 8: **Fullstack App with CPU HPA + Karpenter Node Scaling**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ Iâ€™m a fullstack developer deploying scalable infra.  
ğŸ¯ I want to deploy a React + Node.js app where pods scale via HPA and nodes auto-provision via Karpenter.  
ğŸ’» Stack: React, Node.js, AWS EKS, Karpenter, HPA  
ğŸ“ Structure:
fullstack-karpenter-hpa/
â”œâ”€â”€ backend/  
â”œâ”€â”€ frontend/  
â”œâ”€â”€ provisioner.yaml  
â”œâ”€â”€ hpa.yaml  

ğŸ”£ Input: Load spikes to both services  
ğŸ“¤ Output: App scales at both pod and node level  

ğŸŒ Environment: AWS EKS  
ğŸ“œ Constraints: Metrics Server + IAM role for Karpenter  
ğŸ§¾ Output: Traffic â†’ pod â†’ node scale-up  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: Production-ready fullstack app  
ğŸ’¡ I want complete stack with automation  
ğŸš€ Deploy: Helm + kubectl  
ğŸ” Best Practices: Consolidation and warm-up buffer  
ğŸ“¥ Logs: `top pods`, `karpenter`, `HPA` status  
ğŸŒ Language: English
```

---

### âœ… Prompt 9: **Custom Metric Autoscaling Using KEDA**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ I want to scale using external metrics.  
ğŸ¯ I want to install KEDA and configure autoscaling based on Redis queue length.  
ğŸ’» Stack: KEDA, Redis, Node.js  
ğŸ“ Structure:
keda-autoscale/
â”œâ”€â”€ scaledobject.yaml  
â”œâ”€â”€ deployment.yaml  

ğŸ”£ Input: Redis stream backlog  
ğŸ“¤ Output: KEDA scales consumer pods based on queue  

ğŸŒ Environment: EKS with KEDA  
ğŸ“œ Constraints: External scaler config must be valid  
ğŸ§¾ Output: Redis spikes â†’ pod increases  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: Queue-based worker autoscaling  
ğŸ’¡ I want Redis-based trigger example  
ğŸš€ Deploy: Helm + YAML  
ğŸ” Best Practices: Use scale cooldown to prevent thrashing  
ğŸ“¥ Logs: KEDA controller logs  
ğŸŒ Language: English
```

---

### âœ… Prompt 10: **Event-Driven Scaling with AWS Lambda Trigger via KEDA**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ I want to autoscale based on AWS event triggers.  
ğŸ¯ I want to scale a Kubernetes job based on SQS messages using KEDA.  
ğŸ’» Stack: AWS EKS, KEDA, AWS SQS, Node.js  
ğŸ“ Structure:
keda-sqs-scaler/
â”œâ”€â”€ scaledjob.yaml  
â”œâ”€â”€ deployment.yaml  

ğŸ”£ Input: SQS message backlog  
ğŸ“¤ Output: Consumer pods run when messages arrive  

ğŸŒ Environment: AWS EKS, KEDA installed  
ğŸ“œ Constraints: IRSA required for SQS access  
ğŸ§¾ Output: Pod spin-up based on SQS trigger  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: Serverless-like scaling in K8s  
ğŸ’¡ I want SQS scaler + IRSA sample  
ğŸš€ Deploy: Helm  
ğŸ” Best Practices: Limit concurrency  
ğŸ“¥ Logs: `kubectl logs` + KEDA SQS adapter  
ğŸŒ Language: English
```

---

## ğŸ”¥ Week 5: Scaling & Autoscaling â€“ Super Advanced Prompts (11â€“20)

---

### âœ… Prompt 11: **Multi-Zone Karpenter Scaling with Zonal Balancing**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ Iâ€™m optimizing zonal availability.  
ğŸ¯ I want to configure Karpenter to balance node provisioning across multiple availability zones.  
ğŸ’» Stack: AWS EKS, Karpenter, Helm  
ğŸ“ Structure:
karpenter-zonal-balance/
â”œâ”€â”€ provisioner.yaml  

ğŸ”£ Input: Multiple AZs in EKS  
ğŸ“¤ Output: EC2 nodes spread across zones based on demand  

ğŸŒ Environment: AWS EKS with subnets in 3 AZs  
ğŸ“œ Constraints: Proper subnet tagging per zone  
ğŸ§¾ Output: Nodes created in multiple AZs  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: Zonal fault tolerance  
ğŸ’¡ I want Karpenter provisioner example with zone spreading  
ğŸš€ Deploy: Helm + AWS CLI  
ğŸ” Best Practices: Use `topologySpreadConstraints`  
ğŸ“¥ Logs: `karpenter` controller logs and EC2 AZ check  
ğŸŒ Language: English
```

---

### âœ… Prompt 12: **Karpenter + Spot Termination Handling with Lifecycle Hooks**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ I want to gracefully handle spot instance terminations.  
ğŸ¯ I want to deploy an app on Karpenter-provisioned Spot instances that handles termination via lifecycle hook.  
ğŸ’» Stack: AWS EKS, Karpenter, EC2, Node.js  
ğŸ“ Structure:
karpenter-spot-lifecycle/
â”œâ”€â”€ deployment.yaml  
â”œâ”€â”€ lifecycle-handler.js  

ğŸ”£ Input: Spot instance termination notice  
ğŸ“¤ Output: App logs the event and gracefully exits  

ğŸŒ Environment: AWS EKS with Spot provisioning  
ğŸ“œ Constraints: Node must emit termination notice  
ğŸ§¾ Output: Lifecycle hook prints message before termination  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: Stateful apps on Spot  
ğŸ’¡ I want graceful shutdown logic  
ğŸš€ Deploy: Helm  
ğŸ” Best Practices: Use `terminationGracePeriodSeconds`  
ğŸ“¥ Logs: `karpenter` + app logs before termination  
ğŸŒ Language: English
```

---

### âœ… Prompt 13: **Pod Disruption Budget (PDB) with HPA + Karpenter**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ I want to prevent aggressive downscaling.  
ğŸ¯ I want to use a Pod Disruption Budget with HPA and Karpenter to protect high-availability backend services.  
ğŸ’» Stack: EKS, HPA, Karpenter, Node.js  
ğŸ“ Structure:
pdb-karpenter-hpa/
â”œâ”€â”€ deployment.yaml  
â”œâ”€â”€ hpa.yaml  
â”œâ”€â”€ pdb.yaml  

ğŸ”£ Input: PDB `minAvailable: 2`, HPA up to 5 pods  
ğŸ“¤ Output: Karpenter and HPA scale, PDB ensures stability  

ğŸŒ Environment: AWS EKS  
ğŸ“œ Constraints: Minimum pod availability enforced  
ğŸ§¾ Output: Evictions restricted, PDB honored  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: HA backend workloads  
ğŸ’¡ I want full deployment with PDB + scaling  
ğŸš€ Deploy: kubectl  
ğŸ” Best Practices: PDB + readiness probe = safer scaling  
ğŸ“¥ Logs: Eviction events + PDB status  
ğŸŒ Language: English
```

---

### âœ… Prompt 14: **Auto-Scaling Job Queue with ScaledJob and Workload Identity**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ I want event-driven scaling for background jobs.  
ğŸ¯ I want to use KEDAâ€™s ScaledJob on EKS to scale workers based on an SQS queue using IAM Roles for Service Accounts.  
ğŸ’» Stack: AWS EKS, KEDA, SQS, Node.js  
ğŸ“ Structure:
keda-scaledjob-irsa/
â”œâ”€â”€ scaledjob.yaml  
â”œâ”€â”€ serviceaccount.yaml  
â”œâ”€â”€ sqs-listener.js  

ğŸ”£ Input: SQS queue depth  
ğŸ“¤ Output: New pods spawn when messages arrive  

ğŸŒ Environment: AWS EKS, IRSA  
ğŸ“œ Constraints: SQS IAM role attached to SA  
ğŸ§¾ Output: Queue messages processed at scale  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: Background job system  
ğŸ’¡ I want ScaledJob YAML + IRSA setup  
ğŸš€ Deploy: Helm + AWS CLI  
ğŸ” Best Practices: Use `maxReplicaCount` + timeouts  
ğŸ“¥ Logs: KEDA controller + pod logs  
ğŸŒ Language: English
```

---

### âœ… Prompt 15: **Fullstack Autoscaling with Ingress-Based Traffic Split**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ Iâ€™m testing A/B autoscaling.  
ğŸ¯ I want to deploy 2 versions of a fullstack app (v1, v2), route traffic via Ingress, and autoscale each version separately using HPA.  
ğŸ’» Stack: React, Node.js, NGINX Ingress, HPA  
ğŸ“ Structure:
fullstack-ingress-ab-hpa/
â”œâ”€â”€ v1-backend/  
â”œâ”€â”€ v2-backend/  
â”œâ”€â”€ frontend/  
â”œâ”€â”€ hpa-v1.yaml  
â”œâ”€â”€ hpa-v2.yaml  
â”œâ”€â”€ ingress.yaml  

ğŸ”£ Input: 80% traffic to v1, 20% to v2  
ğŸ“¤ Output: HPA scales pods for both versions independently  

ğŸŒ Environment: AWS EKS with NGINX Ingress  
ğŸ“œ Constraints: Must track Ingress traffic to both backends  
ğŸ§¾ Output: 2 HPA resources with different scaling behavior  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: Feature testing and canary autoscaling  
ğŸ’¡ I want Ingress-based A/B autoscaling demo  
ğŸš€ Deploy: Helm + YAML  
ğŸ” Best Practices: Use weighted routing + metrics  
ğŸ“¥ Logs: HPA status per version  
ğŸŒ Language: English
```

---

### âœ… Prompt 16: **GPU-Based Auto-Scaling with Karpenter for ML Workloads**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ I want to autoscale GPU resources for training jobs.  
ğŸ¯ I want Karpenter to provision EC2 GPU instances dynamically for a TensorFlow job.  
ğŸ’» Stack: Karpenter, TensorFlow, AWS EKS, EC2 GPU  
ğŸ“ Structure:
karpenter-gpu-autoscale/
â”œâ”€â”€ provisioner-gpu.yaml  
â”œâ”€â”€ training-job.yaml  

ğŸ”£ Input: TensorFlow pod with `nvidia.com/gpu` request  
ğŸ“¤ Output: Karpenter schedules pod on new GPU node  

ğŸŒ Environment: AWS EKS, GPU node support  
ğŸ“œ Constraints: AMI + drivers must support NVIDIA  
ğŸ§¾ Output: ML job completes on provisioned GPU  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: ML training on-demand  
ğŸ’¡ I want GPU auto-scaler with Karpenter  
ğŸš€ Deploy: Helm + kubectl  
ğŸ” Best Practices: Use tolerations and node selectors  
ğŸ“¥ Logs: GPU metrics + Karpenter scaling logs  
ğŸŒ Language: English
```

---

### âœ… Prompt 17: **Reactive Scaling with Prometheus + AlertManager Trigger**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ I want scaling based on alert conditions.  
ğŸ¯ I want to scale a service when Prometheus alerts exceed a threshold using KEDAâ€™s Prometheus scaler.  
ğŸ’» Stack: Prometheus, KEDA, EKS  
ğŸ“ Structure:
prometheus-keda-scale/
â”œâ”€â”€ scaledobject.yaml  
â”œâ”€â”€ prometheus-rule.yaml  

ğŸ”£ Input: CPU usage alert  
ğŸ“¤ Output: Trigger pod scaling when alert fires  

ğŸŒ Environment: AWS EKS  
ğŸ“œ Constraints: Prometheus metrics must be exposed  
ğŸ§¾ Output: Alert triggers scaler â†’ pod increase  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: Reactive alert-based autoscaling  
ğŸ’¡ I want alert rule + scaled object setup  
ğŸš€ Deploy: Helm  
ğŸ” Best Practices: Avoid alert flapping  
ğŸ“¥ Logs: AlertManager + KEDA scaler logs  
ğŸŒ Language: English
```

---

### âœ… Prompt 18: **Autoscale CronJobs Based on Queue Length with KEDA**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ I want to scale CronJob runs based on queue backlog.  
ğŸ¯ I want to use KEDA to schedule and scale a Kubernetes CronJob dynamically based on Redis length.  
ğŸ’» Stack: EKS, KEDA, Redis, Node.js  
ğŸ“ Structure:
scaled-cronjob-redis/
â”œâ”€â”€ scaledjob.yaml  
â”œâ”€â”€ cronjob-template.yaml  

ğŸ”£ Input: Redis queue > 100  
ğŸ“¤ Output: CronJobs run in parallel when load is high  

ğŸŒ Environment: AWS EKS  
ğŸ“œ Constraints: Redis URI in secret  
ğŸ§¾ Output: Scaled job runs on trigger  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: Queue-based scheduled workloads  
ğŸ’¡ I want CronJob template with KEDA scaler  
ğŸš€ Deploy: Helm + YAML  
ğŸ” Best Practices: Use TTLSecondsAfterFinished  
ğŸ“¥ Logs: `kubectl get jobs` + `kubectl logs`  
ğŸŒ Language: English
```

---

### âœ… Prompt 19: **Auto-Scaling StatefulSet with Custom Metrics**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ I want to autoscale a StatefulSet.  
ğŸ¯ I want to scale MongoDB StatefulSet based on disk IOPS metrics using custom Prometheus metrics.  
ğŸ’» Stack: EKS, StatefulSet, Prometheus Adapter  
ğŸ“ Structure:
stateful-hpa-custom/
â”œâ”€â”€ statefulset.yaml  
â”œâ”€â”€ hpa.yaml  

ğŸ”£ Input: IOPS custom metric  
ğŸ“¤ Output: Replica scale-up if metric breached  

ğŸŒ Environment: AWS EKS + Prometheus  
ğŸ“œ Constraints: Persistent volume metrics must be exposed  
ğŸ§¾ Output: StatefulSet scaled based on disk pressure  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: Stateful DB under load  
ğŸ’¡ I want StatefulSet scaling config  
ğŸš€ Deploy: kubectl  
ğŸ” Best Practices: Respect pod identity  
ğŸ“¥ Logs: Prometheus metrics + HPA status  
ğŸŒ Language: English
```

---

### âœ… Prompt 20: **Karpenter + HPA + VPA Hybrid Autoscaling Simulation**  
**ğŸ§  Level:** Super Advanced  
```
ğŸ”§ I want full hybrid autoscaling.  
ğŸ¯ I want to deploy an app that uses HPA for pod scaling, Karpenter for node scaling, and VPA to tune resource requests.  
ğŸ’» Stack: EKS, Karpenter, HPA, VPA, Node.js  
ğŸ“ Structure:
hybrid-autoscale/
â”œâ”€â”€ deployment.yaml  
â”œâ”€â”€ hpa.yaml  
â”œâ”€â”€ vpa.yaml  
â”œâ”€â”€ provisioner.yaml  

ğŸ”£ Input: CPU stress test  
ğŸ“¤ Output: VPA adjusts requests, HPA adds pods, Karpenter adds nodes  

ğŸŒ Environment: AWS EKS  
ğŸ“œ Constraints: VPA must be in recommendation mode  
ğŸ§¾ Output: Logs from all 3 autoscalers  
ğŸ”§ Experience Level: Super Advanced  
ğŸ¢ Use Case: Intelligent self-scaling infrastructure  
ğŸ’¡ I want full hybrid setup with inline comments  
ğŸš€ Deploy: Helm + kubectl  
ğŸ” Best Practices: VPA in recommend-only for HPA compatibility  
ğŸ“¥ Logs: HPA + VPA + Karpenter logs  
ğŸŒ Language: English
```

---

